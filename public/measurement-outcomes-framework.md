# Nebula Maze - Measurement & Outcomes Framework

## Comprehensive Guide to Assessing Educational Impact

*For Educators, Administrators, and Researchers*

---

## Table of Contents

1. [Overview](#overview)
2. [Theory of Change](#theory-of-change)
3. [Measurement Domains](#measurement-domains)
4. [Data Collection Methods](#data-collection-methods)
5. [Assessment Tools & Templates](#assessment-tools--templates)
6. [Implementation Timeline](#implementation-timeline)
7. [Data Analysis Guide](#data-analysis-guide)
8. [Reporting Templates](#reporting-templates)
9. [Research Partnership Opportunities](#research-partnership-opportunities)

---

## Overview

### Purpose of This Framework

This measurement framework helps educators and administrators:
- **Evaluate** the effectiveness of Nebula Maze in your context
- **Document** learning outcomes for stakeholders (parents, admin, funders)
- **Improve** implementation based on data
- **Demonstrate** return on investment (ROI)
- **Contribute** to broader research on game-based learning

### Measurement Philosophy

**Balanced Approach**: Combine quantitative metrics (test scores, usage data) with qualitative insights (student voice, teacher observations)

**Growth-Oriented**: Focus on improvement over time, not just absolute performance

**Context-Aware**: Recognize that effectiveness varies by student population, implementation model, and educational goals

**Actionable**: Collect data that informs decisions and improvements, not just for compliance

---

## Theory of Change

### Logic Model

**Inputs** â†’ **Activities** â†’ **Outputs** â†’ **Short-Term Outcomes** â†’ **Medium-Term Outcomes** â†’ **Long-Term Outcomes**

#### Inputs
- Nebula Maze platform and content
- Teacher training and support
- Student access to devices/internet
- Class time allocated to gameplay
- School/district commitment

#### Activities
- Students play game 2-4x per week, 20-40 minutes per session
- Teachers integrate game into curriculum
- Students encounter standards-aligned questions
- Teachers monitor progress and adjust instruction
- Students receive immediate feedback

#### Outputs
- Number of students using platform
- Hours of engagement per student
- Questions attempted and answered
- Standards covered
- Teacher participation rate

#### Short-Term Outcomes (0-3 months)
- Increased student engagement with content
- Improved attitudes toward challenging subjects
- Basic skill development (specific standards)
- Familiarity with game mechanics

#### Medium-Term Outcomes (3-9 months)
- Measurable gains in standards mastery
- Development of problem-solving strategies
- Increased persistence and resilience
- Transfer of skills to other contexts
- Improved formative assessment performance

#### Long-Term Outcomes (9+ months)
- Sustained academic achievement gains
- Improved summative assessment scores (state tests, benchmarks)
- Development of 21st-century skills (critical thinking, self-directed learning)
- Positive mindset toward learning and challenges
- Equitable outcomes across student subgroups

---

## Measurement Domains

### Domain 1: Academic Achievement

**What We Measure**: Mastery of content standards (math, science, social studies, ELA)

**Indicators**:
- % of standards mastered (>70% accuracy on in-game questions)
- Growth in standards mastery over time
- Alignment with external assessments (benchmark tests, state tests)
- Transfer to classroom performance (homework, quizzes, projects)

**Data Sources**:
- In-game performance data
- Pre/post assessments
- Classroom grades
- Standardized test scores

---

### Domain 2: Engagement & Motivation

**What We Measure**: Student engagement, intrinsic motivation, attitudes toward learning

**Indicators**:
- Time on task (minutes played per week)
- Voluntary usage (play outside required time)
- Persistence (continues despite difficult questions)
- Self-reported enjoyment and interest
- Attendance on game days

**Data Sources**:
- Usage analytics (automatic)
- Student surveys
- Teacher observations
- Attendance records

---

### Domain 3: Social-Emotional Learning (SEL)

**What We Measure**: Development of SEL competencies (CASEL framework)

**Indicators**:
- Self-awareness: Recognizing emotions, self-assessment accuracy
- Self-management: Goal-setting, perseverance, handling frustration
- Social awareness: Empathy, perspective-taking (in game scenarios)
- Relationship skills: Collaboration (if using multiplayer features)
- Responsible decision-making: Ethical reasoning in game choices

**Data Sources**:
- SEL question performance in game
- Pre/post SEL assessments (e.g., DESSA, CASEL screeners)
- Teacher observations (SEL rubrics)
- Student reflections

---

### Domain 4: Cognitive & Metacognitive Skills

**What We Measure**: Problem-solving, critical thinking, strategic thinking, self-regulation

**Indicators**:
- Strategy use (observing student approaches to challenges)
- Metacognitive awareness (self-assessment accuracy)
- Transfer of strategies (applying game strategies to other contexts)
- Improvement in executive function skills

**Data Sources**:
- Teacher observations
- Student reflections (thinking journals)
- Gameplay analytics (patterns of behavior)
- Performance tasks

---

### Domain 5: Equity & Inclusion

**What We Measure**: Equitable access, outcomes, and engagement across student groups

**Indicators**:
- Access: % of students with regular access to game
- Usage: Comparable engagement across subgroups (race, gender, SES, ELL, SpEd)
- Outcomes: Comparable growth across subgroups
- Perception: All students feel the game is for them

**Data Sources**:
- Usage data disaggregated by subgroup
- Achievement data disaggregated by subgroup
- Student surveys (sense of belonging)
- Focus groups with underrepresented students

---

## Data Collection Methods

### 1. In-Game Analytics (Automatic)

**Collected Automatically**:
- Questions attempted, correct/incorrect
- Time spent (per session, per question)
- Standards covered
- Difficulty levels attempted
- Progression through game (levels completed)
- Patterns (e.g., time of day, day of week)

**Advantages**: No additional work, real-time, granular, longitudinal

**Limitations**: Only measures what happens in-game, not transfer to other contexts

**Access**: Teacher dashboard (all license tiers), exportable reports (School License+)

---

### 2. Pre/Post Assessments

**Purpose**: Measure growth in content knowledge

**Recommended Schedule**:
- **Pre-Assessment**: Before starting Nebula Maze (baseline)
- **Mid-Assessment**: After 6-8 weeks (formative)
- **Post-Assessment**: After 12-16 weeks (summative)

**Assessment Options**:

**Option A: Standards-Based Formative Assessments**
- Create or use existing assessments aligned with target standards
- 10-20 questions per assessment
- Multiple-choice or short-answer format
- Examples: District benchmark tests, MAP Growth, iReady

**Option B: Create Custom Aligned Assessments**
- Select 15-20 questions from Nebula Maze question bank
- Administer on paper or via Google Forms
- Ensures perfect alignment with game content

**Analysis**: Compare pre and post scores; calculate growth (e.g., +15% = significant growth)

**Template**: See Section 5 for Pre/Post Assessment Template

---

### 3. Student Surveys

**Purpose**: Assess engagement, motivation, attitudes, SEL, and student voice

**Recommended Schedule**:
- **Baseline**: Before or within first week of use
- **Mid-point**: After 6-8 weeks
- **End**: After 12-16 weeks or end of term

**Survey Domains**:
- Enjoyment and engagement
- Perceived learning value
- Attitudes toward subject (e.g., "I like math")
- Self-efficacy ("I can do this")
- SEL competencies (optional)

**Sample Items**:
1. "I enjoy playing Nebula Maze." (1=Strongly Disagree, 5=Strongly Agree)
2. "Nebula Maze helps me learn [math/science/etc.]."
3. "I feel confident when solving problems in [subject]."
4. "When I struggle with a question, I keep trying."
5. (Open-ended) "What do you like most about Nebula Maze?"

**Template**: See Section 5 for Student Survey Template

---

### 4. Teacher Surveys & Observations

**Purpose**: Assess implementation fidelity, teacher perceptions, and observed student behaviors

**Recommended Schedule**:
- **Baseline**: After initial training, before implementation
- **Mid-point**: After 6-8 weeks
- **End**: After 12-16 weeks

**Survey Domains**:
- Ease of implementation
- Perceived impact on student learning
- Perceived impact on engagement
- Integration with curriculum
- Technical issues
- Professional development needs

**Sample Items**:
1. "Nebula Maze has increased student engagement in my class."
2. "I feel confident integrating Nebula Maze into my lessons."
3. "Technical issues have NOT interfered with implementation." (reverse-coded)
4. (Open-ended) "What has been your biggest success with Nebula Maze?"

**Observation Protocol**:
- Observe students during gameplay (15-20 minutes)
- Note: engagement signs, frustration handling, collaboration, strategy use
- Use checklist or rubric (see template in Section 5)

**Template**: See Section 5 for Teacher Survey & Observation Protocol

---

### 5. Focus Groups & Interviews

**Purpose**: Deep qualitative insights into student and teacher experiences

**Recommended Participants**:
- **Student Focus Groups**: 6-8 students per group, mixed ability levels
- **Teacher Interviews**: Individual or small group (3-4)

**Timing**: Mid-point and/or end of implementation

**Sample Questions**:

**For Students**:
1. "Tell me about a time Nebula Maze helped you learn something."
2. "What's the hardest part of playing the game? How do you handle it?"
3. "How is learning with Nebula Maze different from other ways you learn?"
4. "If you could change one thing about the game, what would it be?"

**For Teachers**:
1. "How has Nebula Maze affected your teaching?"
2. "Can you describe a student for whom the game really worked? Why?"
3. "What barriers have you faced? How did you overcome them?"
4. "Would you recommend Nebula Maze to a colleague? Why or why not?"

**Analysis**: Thematic coding, identify patterns, use quotes for reporting

---

### 6. Classroom Performance Data

**Purpose**: Assess transfer of learning to non-game contexts

**Data Sources**:
- Homework completion and accuracy (in target subject)
- Quiz and test scores (on aligned standards)
- Classroom participation (teacher observation)
- Performance tasks and projects

**Analysis**: Compare performance before and during Nebula Maze use; look for improvement trends

---

### 7. Standardized Test Data (Long-Term)

**Purpose**: Assess impact on high-stakes assessments

**Data Sources**:
- State tests (e.g., PARCC, SBAC, state-specific tests)
- National assessments (e.g., MAP Growth, iReady, NWEA)

**Analysis**: Compare cohorts (students who used Nebula Maze vs. those who didn't); control for baseline differences

**Note**: Requires longer implementation (6+ months) and larger sample size for statistical significance

---

## Assessment Tools & Templates

### Template 1: Pre/Post Content Assessment

**Format**: Google Forms, paper-based, or district assessment platform

**Structure**:
- 15-20 multiple-choice questions
- Aligned with target standards (select from Nebula Maze question bank or use district items)
- Same questions on pre and post (or parallel forms)

**Scoring**:
- Calculate % correct for each student
- Calculate growth: (Post % - Pre %) = Growth %
- Aggregate for class: Average growth

**Sample Item**:
> **Standard**: CCSS.MATH.CONTENT.6.RP.A.1 (Ratios)
>
> **Question**: Your exploration team travels 45 miles in 3 hours. At this rate, how far will you travel in 5 hours?
> - A) 60 miles
> - B) 75 miles
> - C) 90 miles
> - D) 100 miles
>
> **Correct Answer**: B

**Download**: [Link to Google Forms template]

---

### Template 2: Student Survey (Baseline, Mid, End)

**Format**: Google Forms, 5-10 minutes to complete

**Sections**:
1. **Engagement** (5 items)
2. **Learning Value** (4 items)
3. **Subject Attitude** (4 items)
4. **Self-Efficacy** (3 items)
5. **Open-Ended Feedback** (2 questions)

**Sample Items**:

*Engagement*:
1. I enjoy playing Nebula Maze. (1-5 scale)
2. Time goes by quickly when I'm playing. (1-5 scale)
3. I look forward to Nebula Maze days. (1-5 scale)

*Learning Value*:
4. Nebula Maze helps me learn [math/science/etc.]. (1-5 scale)
5. The questions in the game help me prepare for tests. (1-5 scale)

*Subject Attitude*:
6. I like [math/science/etc.]. (1-5 scale)
7. I am good at [math/science/etc.]. (1-5 scale)

*Self-Efficacy*:
8. I can solve difficult problems if I try hard enough. (1-5 scale)
9. When I struggle with a question, I keep trying. (1-5 scale)

*Open-Ended*:
10. What do you like most about Nebula Maze?
11. What could make Nebula Maze better?

**Scoring**:
- Calculate mean scores per section
- Compare baseline, mid, and end scores
- Look for trends (increasing engagement, improving attitudes)

**Download**: [Link to Google Forms template]

---

### Template 3: Teacher Survey

**Format**: Google Forms, 10-15 minutes to complete

**Sections**:
1. **Implementation Fidelity** (5 items)
2. **Perceived Impact - Academic** (5 items)
3. **Perceived Impact - Engagement** (4 items)
4. **Usability & Support** (4 items)
5. **Open-Ended Feedback** (3 questions)

**Sample Items**:

*Implementation*:
1. How many times per week do your students use Nebula Maze? (dropdown)
2. Average session length: (dropdown: <20 min, 20-30 min, 30-40 min, >40 min)
3. I have integrated Nebula Maze into my regular curriculum. (1-5 scale)

*Perceived Impact - Academic*:
4. Nebula Maze has improved student understanding of [subject] concepts. (1-5)
5. Students' performance on aligned assessments has improved. (1-5)

*Perceived Impact - Engagement*:
6. Student engagement in [subject] has increased since using Nebula Maze. (1-5)
7. More students voluntarily participate in [subject] activities. (1-5)

*Usability*:
8. Nebula Maze is easy to use in my classroom. (1-5)
9. I received adequate training and support. (1-5)

*Open-Ended*:
10. What has been your biggest success with Nebula Maze?
11. What challenges have you faced?
12. Would you recommend Nebula Maze to a colleague? Why or why not?

**Download**: [Link to Google Forms template]

---

### Template 4: Classroom Observation Protocol

**Purpose**: Structured observation of students during gameplay

**Duration**: 15-20 minutes

**Observer**: Teacher, administrator, or instructional coach

**Focus Areas**:
1. **Engagement Indicators**
2. **Problem-Solving Behaviors**
3. **SEL Competencies**
4. **Collaboration** (if applicable)

**Observation Checklist**:

| Indicator | Observed? | Notes |
|-----------|-----------|-------|
| **Engagement** |
| Students appear focused on game | â˜ Yes â˜ No | |
| Minimal off-task behavior | â˜ Yes â˜ No | |
| Time on task >80% of observation | â˜ Yes â˜ No | |
| **Problem-Solving** |
| Students read questions carefully | â˜ Yes â˜ No | |
| Students use strategies (e.g., eliminate wrong answers) | â˜ Yes â˜ No | |
| Students revisit incorrect questions | â˜ Yes â˜ No | |
| **SEL** |
| Students handle frustration productively | â˜ Yes â˜ No | |
| Students celebrate successes (self or others) | â˜ Yes â˜ No | |
| Students persist despite difficulty | â˜ Yes â˜ No | |
| **Collaboration** (if applicable) |
| Students discuss strategies with peers | â˜ Yes â˜ No | |
| Students support each other | â˜ Yes â˜ No | |

**Qualitative Notes**: (Space for observer to record notable moments, quotes, concerns)

**Download**: [Link to PDF template]

---

### Template 5: Parent/Guardian Feedback Form

**Purpose**: Gather parent perspectives on child's experience

**Format**: Google Forms or paper (sent home), 5 minutes

**Sample Items**:
1. Has your child talked about Nebula Maze at home? â˜ Yes â˜ No
2. If yes, what have they said? (open-ended)
3. Have you noticed changes in your child's attitude toward [subject]? (open-ended)
4. My child seems more engaged in learning since using Nebula Maze. (1-5 scale)
5. Additional comments: (open-ended)

**Use**: Optional, provides additional stakeholder perspective

**Download**: [Link to Google Forms template]

---

## Implementation Timeline

### Timeline for Comprehensive Evaluation

**Recommended Duration**: One semester (16-18 weeks) or full academic year

### Phase 1: Pre-Implementation (Weeks 1-2)

**Week 1**:
- â˜ Administer pre-assessments (content knowledge)
- â˜ Administer baseline student survey
- â˜ Administer baseline teacher survey
- â˜ Set evaluation goals and success metrics

**Week 2**:
- â˜ Teacher training on Nebula Maze and data collection
- â˜ Technical setup and testing
- â˜ Collect baseline classroom performance data (grades from previous unit)

### Phase 2: Implementation (Weeks 3-14)

**Ongoing (Weeks 3-14)**:
- â˜ Students use Nebula Maze 2-4x per week
- â˜ Teachers monitor usage via dashboard weekly
- â˜ In-game data collected automatically

**Mid-Point (Weeks 8-9)**:
- â˜ Administer mid-point student survey
- â˜ Administer mid-point teacher survey
- â˜ Conduct classroom observations (2-3 per class)
- â˜ Hold teacher focus group or interviews
- â˜ Analyze preliminary data, adjust implementation as needed

**Ongoing Data Collection**:
- â˜ Track classroom grades on aligned content
- â˜ Note attendance on game days vs. non-game days

### Phase 3: Post-Implementation (Weeks 15-16)

**Week 15**:
- â˜ Administer post-assessments (content knowledge)
- â˜ Administer end-of-study student survey
- â˜ Administer end-of-study teacher survey
- â˜ Conduct student focus groups (2-3 groups)

**Week 16**:
- â˜ Export all in-game data
- â˜ Collect final classroom performance data
- â˜ Optional: Parent/guardian feedback form

### Phase 4: Analysis & Reporting (Weeks 17-18)

**Week 17**:
- â˜ Analyze quantitative data (pre/post gains, survey results)
- â˜ Analyze qualitative data (open-ended responses, focus groups)
- â˜ Identify trends, successes, areas for improvement

**Week 18**:
- â˜ Create summary report (see templates in Section 8)
- â˜ Share findings with stakeholders (teachers, admin, parents)
- â˜ Plan for next implementation cycle (improvements)

---

## Data Analysis Guide

### Quantitative Analysis

#### 1. Pre/Post Assessment Growth

**Calculate Growth**:
- **Individual Growth**: Post Score % - Pre Score % = Growth %
- **Class Average Growth**: Mean(all student growth scores)
- **Effect Size** (for research): (Mean Post - Mean Pre) / SD Pre

**Interpretation**:
- **Small Growth**: 5-10% increase
- **Moderate Growth**: 10-20% increase
- **Large Growth**: >20% increase

**Disaggregation**: Analyze by subgroup (gender, race, ELL, SpEd) to assess equity

**Example**:
> Pre-assessment class average: 58%
> Post-assessment class average: 74%
> Growth: +16% (moderate growth, educationally significant)

---

#### 2. In-Game Performance Trends

**Key Metrics**:
- **Accuracy Rate**: % of questions answered correctly (target: >70%)
- **Standards Mastery**: % of standards where student achieves 70%+ accuracy
- **Engagement**: Average minutes per week (target: 60-120 minutes)
- **Growth Over Time**: Improvement in accuracy from Week 1 to Week 12

**Visualizations**:
- Line graph: Accuracy over time (weeks)
- Bar chart: Standards mastery by student or class
- Heat map: Engagement by student and week

---

#### 3. Survey Analysis

**Likert Scale Items** (1-5):
- Calculate mean scores per item and per section
- Compare across time points (baseline, mid, end)
- Look for trends: Are scores increasing, stable, or decreasing?

**Interpretation** (1-5 scale):
- 1.0-2.0 = Negative/Low
- 2.1-3.0 = Neutral/Moderate
- 3.1-4.0 = Positive
- 4.1-5.0 = Highly Positive

**Statistical Tests** (optional, for research):
- Paired t-test: Compare baseline to post scores
- Effect size (Cohen's d): Standardized measure of change

**Example**:
> Survey Item: "I enjoy playing Nebula Maze."
> Baseline: Mean = 3.8 (Positive)
> Mid: Mean = 4.2 (Highly Positive)
> End: Mean = 4.1 (Highly Positive)
> Interpretation: Sustained high enjoyment throughout

---

### Qualitative Analysis

#### Thematic Coding

**Process**:
1. **Read All Responses**: Get a sense of the whole dataset
2. **Identify Themes**: What patterns emerge? (e.g., "engagement," "difficulty," "fun")
3. **Code Responses**: Tag each response with relevant theme(s)
4. **Count Frequencies**: How often does each theme appear?
5. **Select Quotes**: Choose representative quotes for reporting

**Common Themes** (from pilot data):

**Positive**:
- Engagement/Fun ("It's fun and doesn't feel like learning")
- Helps Learning ("I understand math better now")
- Motivation ("I actually want to do homework now")

**Challenges**:
- Difficulty ("Some questions are too hard")
- Technical Issues ("Sometimes it's slow to load")
- Time ("Wish we had more time to play")

**Use Themes To**:
- Explain quantitative findings
- Identify areas for improvement
- Generate compelling narratives for stakeholders

---

## Reporting Templates

### Report 1: Executive Summary (1-2 pages)

**Audience**: Administrators, funders, board members

**Sections**:
1. **Overview**: What was implemented, when, with whom
2. **Key Findings** (3-5 bullet points):
   - Academic growth (e.g., "+16% on aligned assessments")
   - Engagement (e.g., "92% of students report enjoying the game")
   - Equity (e.g., "ELL students showed comparable growth")
3. **Success Stories**: 1-2 compelling quotes or anecdotes
4. **Recommendations**: Next steps for scaling or improving

**Example**:
> **Key Finding**: Students using Nebula Maze demonstrated a 16% average improvement on standards-aligned assessments over 16 weeks, with all subgroups showing positive growth.

---

### Report 2: Detailed Findings Report (5-10 pages)

**Audience**: Teachers, instructional coaches, researchers

**Sections**:
1. **Introduction & Context**
2. **Methodology**: What data was collected, when, how
3. **Findings by Domain**:
   - Academic Achievement (data + charts)
   - Engagement (data + charts)
   - SEL (data + charts)
   - Equity (data + charts)
4. **Qualitative Insights**: Themes, quotes, observations
5. **Discussion**: What do these findings mean?
6. **Limitations**: What couldn't be measured, caveats
7. **Recommendations**: Actionable next steps
8. **Appendices**: Survey instruments, raw data tables

---

### Report 3: Data Dashboard (Visual)

**Audience**: All stakeholders

**Format**: Infographic, PowerPoint slide, or interactive dashboard

**Visuals**:
- **Academic Growth**: Bar chart showing pre/post gains
- **Engagement**: Usage stats (total hours, sessions, students)
- **Student Voice**: Word cloud of "What do you like most?"
- **Teacher Feedback**: % agreeing with "Nebula Maze improved learning"
- **Equity**: Growth by subgroup (side-by-side bars)

**Tools**: Canva, Google Slides, Tableau, Microsoft Power BI

---

### Report 4: Case Study Narrative

**Audience**: Prospective users, marketing, grants

**Format**: Story-driven, 2-3 pages

**Structure**:
1. **Context**: School/district background, challenges faced
2. **Implementation**: How Nebula Maze was used
3. **Results**: Quantitative and qualitative outcomes
4. **Spotlight**: Individual student or teacher story
5. **Lessons Learned**: What worked, what didn't
6. **Conclusion**: Impact statement

**Example Opening**:
> "When Jefferson Middle School implemented Nebula Maze in Fall 2025, only 42% of 7th graders were proficient in math. By spring, that number had risen to 61%, with the strongest gains among students who had previously struggled..."

---

## Research Partnership Opportunities

### Contribute to Broader Research

We welcome partnerships with educators and researchers to study Nebula Maze's impact.

**Opportunities**:
- **Efficacy Studies**: Randomized controlled trials, quasi-experimental designs
- **Implementation Science**: What conditions support successful adoption?
- **Equity Research**: How does the game affect opportunity gaps?
- **SEL Outcomes**: Measuring social-emotional development
- **Cognitive Science**: How does game-based learning work?

**Benefits of Partnering**:
- Access to enhanced data and analytics
- Co-authorship on publications
- Presentation opportunities at conferences
- Recognition in educator community
- Contribute to evidence base

**Contact**: research@nebula-maze.web.app

---

## Conclusion

This measurement framework provides a comprehensive approach to assessing Nebula Maze's impact. Remember:

âœ… **Start Small**: Don't try to measure everything at once. Pick 2-3 key metrics.

âœ… **Use Data**: Let findings inform your implementation. Adjust based on what you learn.

âœ… **Share Success**: Celebrate wins with students, colleagues, and stakeholders.

âœ… **Iterate**: Each implementation cycle should improve based on prior data.

**Questions or need support with evaluation?**
ðŸ“§ measurement@nebula-maze.web.app

---

*Framework Version 1.0 - January 2026*
